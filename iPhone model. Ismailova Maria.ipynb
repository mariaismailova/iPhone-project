{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iPhone or not iPhone?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Maria Ismailova\n",
    "GSOM, June 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# loading the libraries\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from classification_models.resnet import ResNet18\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Image preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ImageDataGenerator we will generate batches of tensor image data with real-time data augmentation; the data will be looped over. ImageDataGenerator will be used for all three samples: train sample, test sample and validation sample. The parameters will be as following:\n",
    "\n",
    "- rotation_range = 60: degree range for random rotations\n",
    "- width_shift_range = 0.5: number of pixels from interval\n",
    "- height_shift_range = 0.5: number of pixels from interval\n",
    "- shear_range = 0.3: shear angle in counter-clockwise direction in degrees\n",
    "- zoom_range = 0.3: range for random zoom\n",
    "- vertical_flip = True: randomly flip inputs vertically\n",
    "- rescale = 1./255: multiplying the data by this value\n",
    "- fill_mode = 'nearest': points outside the boundaries of the input are filled according to the given mode ('nearest': aaaaaaaa|abcd|dddddddd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming data with ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range = 60,\n",
    "        width_shift_range = 0.5,\n",
    "        height_shift_range = 0.5,\n",
    "        shear_range = 0.3,\n",
    "        zoom_range = 0.3,\n",
    "        vertical_flip = True,\n",
    "        rescale = 1./255,\n",
    "        fill_mode = 'nearest')\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "        rotation_range = 60,\n",
    "        width_shift_range = 0.5,\n",
    "        height_shift_range = 0.5,\n",
    "        shear_range = 0.3,\n",
    "        zoom_range = 0.3,\n",
    "        vertical_flip = True,\n",
    "        rescale = 1./255,\n",
    "        fill_mode = 'nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PyTorch's DataLoader class for accepting created generator; with following parameters:\n",
    "\n",
    "- target_size = (224, 224): the dimensions to which all images found will be resized\n",
    "- batch_size = 128: denotes the number of samples contained in each generated batch\n",
    "- shuffle = False: keeping linear exploration scheme\n",
    "- class_mode = binary: 1D numpy array of binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48684 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# loading the data\n",
    "\n",
    "train_generator = val_datagen.flow_from_directory(\n",
    "        'full dataset/train',  \n",
    "        target_size = (224, 224),  \n",
    "        batch_size = 128,\n",
    "        class_mode = 'binary')\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "        'full dataset/val',  \n",
    "        target_size = (224, 224),  \n",
    "        batch_size = 128,\n",
    "        class_mode = 'binary')\n",
    "\n",
    "test_generator = val_datagen.flow_from_directory(\n",
    "        'full dataset/test',  \n",
    "        target_size = (224, 224),  \n",
    "        batch_size = 128,\n",
    "        shuffle = False,\n",
    "        class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'An': 0, 'Ip': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# putting indeces for classes\n",
    "\n",
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ResNet running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a model we will use Residual Neural Network with 18 layers (ResNet18) with the following parameters:\n",
    "\n",
    "- include_top is True: the fully-connected layer at the top of the network is included\n",
    "- weights are imagent: pre-trained on ImageNet\n",
    "- as include_top is True, input_shape = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/qubvel/classification_models.git\n",
      "  Cloning https://github.com/qubvel/classification_models.git to /tmp/pip-req-build-elp4u0_p\n",
      "Requirement already satisfied (use --upgrade to upgrade): image-classifiers==0.2.2 from git+https://github.com/qubvel/classification_models.git in ./anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: keras>=2.1.0 in ./anaconda3/lib/python3.6/site-packages (from image-classifiers==0.2.2) (2.2.4)\n",
      "Requirement already satisfied: h5py in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2) (2.9.0)\n",
      "Requirement already satisfied: pyyaml in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2) (5.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2) (1.16.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2) (1.2.1)\n",
      "Building wheels for collected packages: image-classifiers\n",
      "  Building wheel for image-classifiers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-m3g7sry1/wheels/de/2b/fd/29a6d33edb8c28bc7d94e95ea1d39c9a218ac500a3cfb1b197\n",
      "Successfully built image-classifiers\n"
     ]
    }
   ],
   "source": [
    "# installing residual network\n",
    "\n",
    "!pip install git+https://github.com/qubvel/classification_models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shipinskaya_sofi/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# loading pretrained residual network\n",
    "\n",
    "model = ResNet18(input_shape = (224, 224, 3), include_top = True, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the last layer\n",
    "\n",
    "model.layers.pop()\n",
    "last = model.layers[-1].output\n",
    "x = Dense (2, activation = \"softmax\")(last)\n",
    "model = Model(model.input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working with the model we should compile it with following parameters:\n",
    "\n",
    "- loss: a function of error, in our case it is sparse categorical crossentropy\n",
    "- optimizer: adam is used, as it shows the best convergence\n",
    "- metrics: by which the quality of the model is considered, in our case it is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing model for futher work\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deviding dataset into parts\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining sample sizes\n",
    "\n",
    "nb_samples_train = len(train_generator.filenames)\n",
    "nb_samples_val = len(val_generator.filenames)\n",
    "nb_samples_test = len(test_generator.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to use adaptive learning rate: to reduce the learning rate as the number of training epochs increases.\n",
    "Here we will use step decay: reduce the learning rate by a constant factor every few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing learning rate\n",
    "\n",
    "def step_decay(epoch):\n",
    "    # initializing learning rate\n",
    "    initial_lrate = 0.0001\n",
    "    # initializing factor by which learning rate reduces after every epoch\n",
    "    drop = 0.5\n",
    "    # initializing number of iterations after which the learning rate should reduce\n",
    "    epochs_drop = 5\n",
    "    # writing a decay function\n",
    "    lrate = initial_lrate*math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should initialize a callback, which will allow to define a function to invoke during program execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing learning rate scheduler callback\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shipinskaya_sofi/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "381/380 [==============================] - 1385s 4s/step - loss: 0.5046 - acc: 0.7610 - val_loss: 0.4579 - val_acc: 0.7910\n",
      "Epoch 2/20\n",
      "381/380 [==============================] - 625s 2s/step - loss: 0.3580 - acc: 0.8313 - val_loss: 0.3230 - val_acc: 0.8420\n",
      "Epoch 3/20\n",
      "381/380 [==============================] - 628s 2s/step - loss: 0.3110 - acc: 0.8549 - val_loss: 0.3482 - val_acc: 0.8420\n",
      "Epoch 4/20\n",
      "381/380 [==============================] - 626s 2s/step - loss: 0.2819 - acc: 0.8698 - val_loss: 0.2821 - val_acc: 0.8600\n",
      "Epoch 5/20\n",
      "381/380 [==============================] - 630s 2s/step - loss: 0.2502 - acc: 0.8868 - val_loss: 0.2568 - val_acc: 0.8750\n",
      "Epoch 6/20\n",
      "381/380 [==============================] - 629s 2s/step - loss: 0.2384 - acc: 0.8929 - val_loss: 0.2514 - val_acc: 0.8940\n",
      "Epoch 7/20\n",
      "381/380 [==============================] - 629s 2s/step - loss: 0.2326 - acc: 0.8968 - val_loss: 0.2490 - val_acc: 0.8870\n",
      "Epoch 8/20\n",
      "381/380 [==============================] - 630s 2s/step - loss: 0.2247 - acc: 0.8993 - val_loss: 0.2166 - val_acc: 0.9020\n",
      "Epoch 9/20\n",
      "381/380 [==============================] - 629s 2s/step - loss: 0.2146 - acc: 0.9048 - val_loss: 0.2489 - val_acc: 0.8995\n",
      "Epoch 10/20\n",
      "381/380 [==============================] - 630s 2s/step - loss: 0.1991 - acc: 0.9123 - val_loss: 0.2231 - val_acc: 0.9045\n",
      "Epoch 11/20\n",
      "381/380 [==============================] - 625s 2s/step - loss: 0.1929 - acc: 0.9155 - val_loss: 0.2113 - val_acc: 0.9015\n",
      "Epoch 12/20\n",
      "381/380 [==============================] - 630s 2s/step - loss: 0.1874 - acc: 0.9182 - val_loss: 0.2291 - val_acc: 0.9020\n",
      "Epoch 13/20\n",
      "381/380 [==============================] - 629s 2s/step - loss: 0.1842 - acc: 0.9194 - val_loss: 0.2259 - val_acc: 0.8940\n",
      "Epoch 14/20\n",
      "381/380 [==============================] - 630s 2s/step - loss: 0.1840 - acc: 0.9192 - val_loss: 0.2337 - val_acc: 0.8975\n",
      "Epoch 15/20\n",
      "381/380 [==============================] - 628s 2s/step - loss: 0.1770 - acc: 0.9233 - val_loss: 0.2133 - val_acc: 0.9030\n",
      "Epoch 16/20\n",
      "381/380 [==============================] - 625s 2s/step - loss: 0.1716 - acc: 0.9246 - val_loss: 0.1912 - val_acc: 0.9175\n",
      "Epoch 17/20\n",
      "381/380 [==============================] - 630s 2s/step - loss: 0.1703 - acc: 0.9257 - val_loss: 0.2117 - val_acc: 0.9150\n",
      "Epoch 18/20\n",
      "381/380 [==============================] - 629s 2s/step - loss: 0.1651 - acc: 0.9281 - val_loss: 0.2004 - val_acc: 0.9120\n",
      "Epoch 19/20\n",
      "381/380 [==============================] - 629s 2s/step - loss: 0.1642 - acc: 0.9286 - val_loss: 0.1937 - val_acc: 0.9090\n",
      "Epoch 20/20\n",
      "381/380 [==============================] - 630s 2s/step - loss: 0.1651 - acc: 0.9291 - val_loss: 0.2175 - val_acc: 0.9065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f13794908>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training ResNet\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch = nb_samples_train/batch_size,\n",
    "        epochs = 20,\n",
    "        validation_data = val_generator,\n",
    "        validation_steps = nb_samples_val/batch_size,\n",
    "        callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "\n",
    "model.save('model2_mi.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving weights\n",
    "\n",
    "model.save_weights('weights2_mi.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert model from keras to tflite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shipinskaya_sofi/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /home/shipinskaya_sofi/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 100 variables.\n",
      "INFO:tensorflow:Converted 100 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46791100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting model\n",
    "\n",
    "converter = lite.TFLiteConverter.from_keras_model_file('model2_mi.h5')\n",
    "tflite_model = converter.convert()\n",
    "open(\"converted_model_mi.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Performance evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function for loading and rescaling images\n",
    "\n",
    "def load_image(img_path):\n",
    "\n",
    "    img = image.load_img(img_path, target_size = (224, 224))\n",
    "    img_tensor = image.img_to_array(img)                    \n",
    "    img_tensor = np.expand_dims(img_tensor, axis = 0)         \n",
    "    img_tensor /= 255.\n",
    "    \n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading converted model\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path = \"converted_model_mi.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "\n",
    "pred = []\n",
    "for subdir, dirs, files in os.walk('full dataset/test'):\n",
    "    for file in files:\n",
    "        input_data = load_image(os.path.join(subdir, file))\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        pred.append(output_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision is: 0.9934829185\n"
     ]
    }
   ],
   "source": [
    "# calculating metrics\n",
    "\n",
    "average_precision = average_precision_score(test_generator.classes, [x[1] for x in pred])\n",
    "print('Average precision is: {0:0.10f}'.format(average_precision))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
